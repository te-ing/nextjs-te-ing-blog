---
title: AI 모델(LLM) 파라미터 이해하기 - 셰프의 관점에서 풀어보는 가이드(번역)
description: AI 모델을 쉽게 이해할 수 있는 Understanding AI Model (LLM) Parameters - A Chef's Guide 포스팅을 번역했습니다.
tags: [AI, LLM, 파인튜닝, 머신러닝]
private: false
---

> 본 포스팅은 [Understanding AI Model (LLM) Parameters - A Chef's Guide](https://dev.to/sreeni5018/understanding-ai-model-llm-parameters-a-chefs-guide-4469) 포스팅을 번역했습니다.

## AI 모델 파라미터란? 쉽게 설명해볼게요

어제 한 친구가 이렇게 물었습니다.
“GPT-4에 1.7조(Trillion) 개의 파라미터가 있다고 하던데, 그게 무슨 뜻이야? 도대체 파라미터가 뭐야?”

아주 좋은 질문이죠. 사실 많은 사람들이 AI와 관련해 1,750억, 1.7조처럼 거대한 숫자를 듣기는 하지만, 그 숫자가 실제로 무엇을 의미하는지는 잘 모릅니다. 그래서 이번에는 제가 친구에게 설명했던 것과 같은 방식으로, 요리에 비유해서 쉽게 설명해보려 합니다.

⸻

일단 우리가 이미 알고 있는 것부터 시작해볼까요?

AI 모델에 대해 이야기할 때, 보통 이런 숫자들을 보게 됩니다.

- GPT-3는 약 1,750억 개의 파라미터를 가지고 있다
- GPT-4는 약 1.7조 개의 파라미터를 가지고 있다
- Claude 3.5 Sonnet은 대략 4,000억 개의 파라미터를 가지고 있다

엄청나게 큰 숫자입니다.
하지만 이게 도대체 무슨 의미일까요?

이 많은 숫자가 곧 **사실(fact)**의 개수일까요?
아니면 문장을 그만큼 저장하고 있다는 뜻일까요?

## 셰프라고 생각해봅시다

우리가 요리를 배우고 있다고 상상해봅시다.
처음에는 레시피를 보고, 재료를 준비하고, 많이 연습해봅니다.

하지만 시간이 지나면 더 이상 레시피만 그대로 따라 하지 않죠.
요리를 이해하게 됩니다.
언제 소금을 더 넣어야 하는지, 얼마나 오래 익혀야 하는지, 어떤 향신료들이 서로 잘 어울리는지를 스스로 알게 됩니다.

![From Training to Deployment](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fq90r5tkhw5xuv54hjck4.png)

AI 모델도 같은 방식으로 작동합니다.

### 원재료(Raw Ingredients) = 학습 데이터(Training Data)

AI 모델을 학습시킬 때 우리는 방대한 양의 텍스트를 제공합니다.
책, 웹사이트, 대화, 코드, 기사 같은 것들이죠.

이것들을 요리의 재료라고 생각해보죠.
밀가루, 향신료, 채소를 많이 가지고 있다고 해서 바로 훌륭한 요리사가 되는 것은 아닙니다.
중요한 것은, 이 재료들을 어떻게 사용하는지 배우는 것입니다.

⸻

### 셰프의 실력(The Chef’s Skill) = 파라미터(Parameters)

여기서 바로 **파라미터**가 등장합니다.

파라미터는 학습 데이터 그 자체가 아니에요.
파라미터는 그 데이터로부터 모델이 배운 결과물입니다.

셰프의 기술, 경험, 그리고 직관이라고도 볼 수 있죠.

예를 들어, 한 셰프가 파스타를 1,000번 만들어본다고 해봅시다. 그러면 그는 다음과 같은 것들을 자연스럽게 알게 됩니다.
• 면을 익힐 때 넣을 소금의 정확한 양
• 풍미를 극대화하기 위해 향신료를 언제 넣어야 하는지
• 불의 세기에 따라 얼마나 오래 조리해야 하는지
• 중간에 문제가 생겼을 때 어떻게 조절해야 하는지

**이 셰프는 파스타 레시피 1,000개를 외운 것이 아닙니다.**
대신 맛있는 파스타가 어떻게 이뤄지는지에 대한 감각과 이해를 갖게 된 것입니다.

그 머릿속에 쌓인 수많은 **작은 판단**과 **미세한 조정들**,
바로 그것이 AI에서 말하는 파라미터입니다.

## 학습은 실제로 어떻게 이루어지나요?

이 부분은 많은 설명들이 놓치는, 가장 중요한 핵심입니다.

파스타를 배우는 요리 학생을 한 명 떠올려봅시다.
이 학생에게는 다음과 같은 과정이 반복됩니다.

Step 1
학생은 현재 알고 있는 방식대로 파스타를 만듭니다.

Step 2
마스터 셰프가 맛을 보고 말합니다.
“소금이 너무 많네.” 또는 “마늘 향이 부족해.”

Step 3
학생은 조리 방법을 조금 수정합니다.
다음에는 소금을 반 티스푼 덜 넣거나, 마늘을 조금 더 일찍 넣어봅니다.

Step 4
수정한 방식으로 다시 파스타를 만듭니다.

Step 5
이 과정을 수천 번 반복합니다.

⸻

1,000번 정도 반복하고 나면,
학생은 더 이상 마스터 셰프가 필요하지 않습니다.

이제는 패턴이 몸에 배어 있어서,
어떻게 하면 맛있는 파스타가 나오는지 본능적으로 알게 됩니다.

### 이것이 바로 AI 학습이 이루어지는 방식입니다

AI 모델은 학습 데이터에 포함된 수십억 개의 문장을 읽습니다.
그리고 각 문장에 대해 다음과 같은 과정을 거칩니다.

1. 다음에 올 단어를 예측합니다: "The cat sat on the \_\_\_"
2. 예측이 맞았는지 확인합니다: 실제 단어는 "mat"
3. 다음에는 더 잘 맞히도록 내부 숫자들(파라미터)을 조정합니다
4. 이 과정을 모든 텍스트에 대해 수십억 번 반복합니다

이 과정을 통해 모델은 문장을 외우는 것이 아니라, 패턴을 학습합니다.

예를 들어,

- 문법 규칙 (주어가 보통 동사보다 앞에 온다)
- 단어 간의 관계 (고양이는 앉고, 새는 난다)
- 문맥의 차이 (강둑의 “bank” vs. 금융기관의 “bank”)
- 추론 패턴 (원인과 결과)

과 같이 학습이 끝나면, 그 1.7조 개의 파라미터 안에는 이런 패턴들이 모두 담기게 됩니다.

이는 마치 모델이 방대한 텍스트를 읽으며 얻은 압축된 지혜와도 같습니다.

## 그렇다면 “1.7조 개의 파라미터”란 정확히 무엇을 의미할까요?

우리가 GPT-4에 1.7조 개의 파라미터가 있다고 말할 때,
그것은 모델이 학습을 통해 얻은 모든 지식을 저장하고 있는, 1.7조 개의 아주 작은 조절 가능한 숫자를 가지고 있다는 뜻입니다.

각각의 파라미터는 하나의 아주 미세한 지식 조각이라고 볼 수 있습니다.

- “이 단어가 나오면, 다음 단어로 저 단어가 나올 확률을 조금 높여라”
- “이 문맥에서는 이런 문장 구조가 더 그럴듯하다”
- “이 개념들과 저 개념은 이런 방식으로 서로 연관되어 있다”

파라미터가 많을수록, 미묘한 패턴과 뉘앙스를 저장할 수 있는 그릇의 크기가 커집니다.

그래서 더 큰 모델일수록 문맥을 더 잘 이해하고, 보다 정교하고 깊이 있는 답변을 내놓을 수 있는 경우가 많습니다.

하지만 여기서 가장 중요한 포인트는 이것입니다.

**파라미터가 많다고 해서 더 많은 사실을 외우고 있다는 뜻은 아닙니다.**
언어 속에 숨어 있는 **패턴**을 이해하는 능력이 더 커졌다는 의미입니다.

## ChatGPT에게 질문을 하면 무슨 일이 일어날까요?

이제 우리가 ChatGPT에 질문을 입력하면, 내부에서는 다음과 같은 일이 벌어집니다.

당신은 음식을 주문하는 손님이고,
AI는 요리를 하는 셰프라고 생각해봅시다.

AI 셰프는 당신의 질문과 완전히 동일한 문장을 데이터베이스에서 찾아보지 않아요.
대신, 앞서 말한 **1.7조 개의 학습된 패턴(파라미터)**을 활용해,
그 순간 즉석에서 새로운 답변을 만들어냅니다.

그래서 ChatGPT는 한 번도 본 적 없는 질문에도 답할 수 있는거에요.

숙련된 셰프가 정확한 레시피가 없어도 새로운 요리를 만들어낼 수 있듯이,
AI 역시 학습을 통해 익힌 패턴을 바탕으로 새로운 답변을 만들어냅니다.

## 작은 모델도 충분히 좋은 이유

이쯤에서 이런 의문이 들 수 있습니다.

> “파라미터가 많을수록 좋다면, 왜 굳이 작은 모델을 쓰는 걸까?”

이렇게 생각해볼 수 있습니다.

매일 먹는 집밥을 만들 때, 반드시 미슐랭 스타 셰프가 필요한 것은 아닙니다.
기본기가 탄탄한 요리사라면 훌륭한 한 끼를 충분히 만들어낼 수 있죠.

GPT-4o 같은 최신 모델(약 2,000억 개의 파라미터)은 더 똑똑하게 설계되어 있습니다.

파라미터 수는 더 적을 수 있지만, 구조가 더 효율적으로 짜여 있어 대부분의 작업에서 매우 좋은 성능을 내죠.

그리고 다음과 같은 장점이 있습니다.

- 더 빠른 응답 속도
- 더 낮은 운영 비용
- 다양한 기기에서 사용하기 쉬움

## 핵심을 요약 하자면,

누군가 당신에게 “AI 파라미터가 뭐야?”라고 묻는다면, 이렇게 말해주면 됩니다.

> 파라미터는 AI 모델 내부에 저장된 ‘학습된 지식’입니다. AI는 수십억 개의 예제를 보면서, 더 정확한 예측을 하기 위해 스스로를 조금씩 계속 수정합니다. 이렇게 수정되면서 쌓인 값들이 바로 파라미터입니다. 파라미터는 사실을 그대로 외운 것이 아닙니다. AI가 언어를 보면서 찾아낸 패턴과 관계입니다.

이것은 요리책을 통째로 외운 사람과,
재료들이 왜 서로 잘 어울리는지를 이해하는 셰프의 차이와도 같아요.

AI는 한 번도 본 적 없는 질문에도
지능적인 답변을 만들어낼 수 있도록 돕는,
**1.7조 개의 아주 작은 ‘이해의 조각’**을 가지고 있는 셈입니다.

이게 전부에요.
이것이 바로 **파라미터**입니다.

## 그런데 잠깐, RAG랑 파인튜닝(Fine-tuning)은 어디에 들어가는 거야?

여기서부터가 더 흥미로워집니다. 친구가 다시 이렇게 물었거든요.

“근데 사람들이 RAG(Retrieval Augmented Generation)나 파인튜닝(fine-tuning) 얘기도 하잖아? 그건 여기서 어떻게 연결되는 거야?”

좋습니다. 요리 비유를 그대로 이어서 설명해볼게요.

### LLM은 ‘냉동식품’ 같은 존재예요

학습이 끝난 LLM(예: GPT-4, Claude)은 품질 좋은 냉동식품이라고 생각하면 이해가 쉬워요.

이미 조리도 끝났고, 맛도 잘 나게 준비되어 있고, 바로 데워 먹을 수 있죠.
그리고 그 냉동식품을 만든 “셰프(= 모델을 학습시킨 회사)”가 이미 대부분의 어려운 일을 해둔 상태입니다.

앞에서 말했던 파라미터는요?

이 냉동식품의 “레시피와 조리법” 자체가 아니라,
**이미 완성되어 얼어붙은 상태로 ‘고정된’ 맛의 규칙/감각(= 학습된 패턴)**이라고 보면 됩니다.

즉, 파라미터는 냉동 상태로 잠겨 있어요.
배포(deployment)된 모델은 보통 그 값들이 그대로 유지됩니다.

하지만 여기서 끝이 아니죠.
이 “냉동식품”을 내 입맛에 맞게 더 맛있게, 더 내 상황에 맞게 만들 수 있는 방법이 있습니다.

대표적으로 두 가지가 있어요.

⸻

### RAG(Retrieval Augmented Generation) = 신선한 재료를 얹는 것

레토르트 파스타가 있다고 해봅시다. 이미 맛있어요.
그런데 “내 스타일”로 더 맛있게 먹고 싶다면 이렇게 하겠죠.

- 데운다
- 위에 **신선한 치즈**를 올린다
- **베이컨**을 곁들인다
- 튀긴 마늘을 추가로 더한다

여기서 중요한 포인트는 이거예요.

레토르트 파스타 자체(= 모델)는 바꾸지 않았습니다.
그냥 주변에 신선한 재료(= 외부 지식/문서/데이터)를 추가해서 더 맛있고, 더 맞춤형으로 만든 거죠.

![](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0ayd8x8rzfid1gkxa4i1.png)

이게 딱 RAG가 하는 일입니다.

- 모델(파라미터)은 그대로 둔 채로
- 질문에 답할 때 필요한 정보를 밖에서 찾아서(retrieval)
- 그 정보를 바탕으로 답변을 **더 탄탄하게 생성(generation)**하는 방식이에요.

즉 RAG는 “모델을 다시 학습시킨다”가 아니라,

마치 이미 잘 만들어진 파스타에,
서빙 직전에 바질과 치즈 같은 신선한 재료를 추가해서 더 맛있고 “지금 상황에 딱 맞게” 완성하는 것과 같습니다.

### 파인튜닝(Fine-tuning) = ‘기존 파스타를 바탕으로 아예 새 메뉴를 만드는 것’

이번엔 조금 다른 상황을 상상해봅시다.
앞에서 말한 “이미 완성된 파스타”를 그냥 마무리(토핑 추가)하는 수준이 아니라, 그 파스타를 기반으로 레시피 자체를 다시 잡아 완전히 다른 메뉴로 바꿔버리는 거예요.

예를 들어 이런 식이죠.

- 크림을 더해 크림 파스타로 바꾸거나
- 해산물과 향신료를 더해서 완전히 다른 풍미의 해산물 파스타를 만들거나
- 채소와 소스를 새로 구성해서 전혀 다른 스타일의 파스타로 리메이크하는 것

즉, 기존 파스타를 “베이스”로 삼긴 하지만,
결과적으로는 새로운 요리를 만들어내는 겁니다.

이게 바로 **Fine-tuning(파인튜닝)**이에요.

![](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9p8mtgufdsg0z6gqo7b8.png)

파인튜닝을 한다는 건, AI 모델의 파라미터 일부를 실제로 “다시 풀어서(unfreeze)” 당신의 데이터로 재학습시키는 것을 의미합니다.

흐름을 요리 비유로 맞춰 보면 이렇게 됩니다.

- 먼저 **베이스 모델**(= 이미 만들어진 파스타/기본 레시피)에서 시작합니다
- 여기에 당신의 구체적인 예시 데이터로 다시 학습시킵니다
  (즉, 새 재료를 넣고 조리 방식을 바꿔가며 반복해서 만들어보는 거죠)
- 그러면 파라미터가 당신의 사용 목적에 맞게 조금씩 조정됩니다
- 결과적으로 맞춤형 모델을 얻게 됩니다

예를 들어, 병원이 GPT-4를 의료 기록 데이터로 파인튜닝해서
의료 특화 AI를 만들 수도 있어요.

이때 모델이 가진 기본 지식(언어 패턴, 일반적인 추론 능력)은 그대로 남아 있지만,
이제는 의료 용어와 문서 패턴을 더 잘 이해하도록 조정된 상태가 되는 거죠.

### 핵심 차이

**RAG** = 모델은 그대로(“고정”), 필요할 때만 신선한 정보를 곁들인다

- 세팅이 빠르다
- 다시 학습시킬 필요가 없다
- 자주 바뀌는 최신 정보(변동되는 문서/정책/회의록 등)를 붙이기에 딱 좋다

**Fine-tuning** = 모델 자체를 ‘다시 풀어서’ 조정한다

- 더 많은 시간과 리소스가 든다
- 실제 파라미터가 변경된다
- 특정 업무/도메인에 특화된 작업이나 지식에 적합하다

두 접근 방식은 공통점이 있습니다.
둘 다 **사전학습된 모델(수천억~수조 개 파라미터를 가진 모델)**을 출발점으로 삼는다는 점이죠.
즉, 기본 모델은 “이미 준비된 요리(혹은 기본 파스타)” 같은 베이스입니다.

다만 목적에 따라 선택이 갈립니다.
\
필요할 때마다 주변에 신선한 재료를 추가해서 완성도를 올릴 것인가? → RAG
\
아예 베이스를 기반으로 새 요리로 재조리할 것인가? → Fine-tuning

---

_참고: 최신 모델의 정확한 파라미터 수는 보통 “추정치”인 경우가 많습니다. 회사들이 공식 수치를 항상 공개하진 않거든요.
하지만 핵심 개념은 같습니다._ **_파라미터는 원본 데이터 자체가 아니라, 데이터로부터 학습된 ‘패턴’_** _을 의미합니다._

감사합니다. \
Sreeni Ramadorai
